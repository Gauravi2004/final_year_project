# final_year_project
This project enables two-way communication for Divyang individuals by converting hand gestures into text and audio using machine learning. It supports alphabetic and word-level gestures with 99.6% accuracy, and also converts responses from the other person into both text and audio formats.
